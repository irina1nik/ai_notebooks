{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNo9aWBalBZ5/t9QwK2s9hr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/irina1nik/ai_notebooks/blob/main/index_from_website_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook makes a question answering chain with a specified website as a context data."
      ],
      "metadata": {
        "id": "EcoEDCsJpBG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up"
      ],
      "metadata": {
        "id": "i3rmZlgg0RsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install dependencies"
      ],
      "metadata": {
        "id": "khCtKh6eDzh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.0.189\n",
        "!pip install pinecone-client\n",
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install nest_asyncio"
      ],
      "metadata": {
        "id": "mxFhFfKzRgpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up OpenAI API key"
      ],
      "metadata": {
        "id": "oU2eHKKCD4lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\""
      ],
      "metadata": {
        "id": "hOiNazy2SSPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up Pinecone API keys"
      ],
      "metadata": {
        "id": "6xUZg07MMvoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone \n",
        "\n",
        "# initialize pinecone\n",
        "pinecone.init(\n",
        "    api_key=\"PINECONE_API_KEY\",  # find at app.pinecone.io\n",
        "    environment=\"ENVIRONMENT_NAME\"  # next to api key in console\n",
        ")"
      ],
      "metadata": {
        "id": "7hai7fChMzw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Index "
      ],
      "metadata": {
        "id": "vCnFhGb40ZxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load data from Web**\n",
        "\n",
        "Extends from the WebBaseLoader, this will load a sitemap from a given URL, and then scrape and load all the pages in the sitemap, returning each page as a document.\n",
        "\n",
        "The scraping is done concurrently, using WebBaseLoader. There are reasonable limits to concurrent requests, defaulting to 2 per second.\n",
        "\n",
        "Link to the [documentation](https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html)"
      ],
      "metadata": {
        "id": "kNrHppQSEQU2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcUaA9YWRS2D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a957320d-f6e7-4513-cec9-361264a14d8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching pages: 100%|##########| 505/505 [03:19<00:00,  2.53it/s]\n"
          ]
        }
      ],
      "source": [
        "# fixes a bug with asyncio and jupyter\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from langchain.document_loaders.sitemap import SitemapLoader\n",
        "\n",
        "loader = SitemapLoader(\n",
        "    \"https://ind.nl/sitemap.xml\",\n",
        "    filter_urls=[\"https://ind.nl/en\"]\n",
        ")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split the text from docs into smaller chunks**\n",
        "\n",
        "There are many ways to split the text. We are using the text splitter that is recommended for generic texts. For more ways to slit the text check the [documentation](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html)"
      ],
      "metadata": {
        "id": "N0Q03G2HG-6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1200,\n",
        "    chunk_overlap  = 200,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "docs_chunks = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "DV9zorYiHAzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create embeddings"
      ],
      "metadata": {
        "id": "BtIKffZiFdpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "KK-LlnZJFPMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a vectorstore**\n",
        "\n",
        "A vectorstore stores Documents and associated embeddings, and provides fast ways to look up relevant Documents by embeddings. \n",
        "\n",
        "There are many ways to create a vectorstore. We are going to use Pinecone. For other types of vectorstores visit the [documentation](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html)\n",
        "\n",
        "First you need to go to [Pinecone](https://www.pinecone.io/) and create an index there. Then type the index name in \"index_name\""
      ],
      "metadata": {
        "id": "5sY05f0sFl_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "index_name = \"ind\"\n",
        "\n",
        "# #create a new index\n",
        "docsearch = Pinecone.from_documents(docs_chunks, embeddings, index_name=index_name)\n",
        "\n",
        "# if you already have an index, you can load it like this\n",
        "#docsearch = Pinecone.from_existing_index(index_name, embeddings)\n"
      ],
      "metadata": {
        "id": "cng7EmkbLyBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you cannot create a Pinecone account, try to use CromaDB. The following code creates a transient in-memory vectorstore. For further information check the [documentation](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/chroma.html). \n",
        "\n",
        "The following code block uses Croma for creating a vectorstore. Uncomment it if you don't have access to pinecone and use it instead."
      ],
      "metadata": {
        "id": "EgbWXad7-dke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.vectorstores import Chroma\n",
        "# docsearch = Chroma.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "P2_WRTDJ-dAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorstore is ready. Let's try to query our docsearch with similarity search"
      ],
      "metadata": {
        "id": "DKpbxw5JRe2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How to get a visa for my partner if I have a highly skilled visa.\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "print(docs[0])"
      ],
      "metadata": {
        "id": "xqqD72sGRbBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making a question answering chain\n",
        "The question answering chain will enable us to generate the answer based on the relevant context chunks. See the [documentation](https://python.langchain.com/en/latest/modules/chains/index_examples/qa_with_sources.html) for more explanation.\n",
        "\n",
        "Additionally, we can return the source documents used to answer the question by specifying an optional parameter when constructing the chain. For more information visit the [documentation](https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html#return-source-documents)"
      ],
      "metadata": {
        "id": "Q_4iKKJ16RCr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qCz7cU_SbvFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "llm=OpenAI()\n",
        "\n",
        "qa_with_sources = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever(), return_source_documents=True)\n",
        "\n",
        "query = \"How to get a visa for my partner if I have a highly skilled visa.\"\n",
        "result = qa_with_sources({\"query\": query})\n",
        "result[\"result\"]"
      ],
      "metadata": {
        "id": "9FcufORSOtJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output source documents that were found for the query"
      ],
      "metadata": {
        "id": "VjOzlTef8e1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"source_documents\"]"
      ],
      "metadata": {
        "id": "maOKWZ7B8TPQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}